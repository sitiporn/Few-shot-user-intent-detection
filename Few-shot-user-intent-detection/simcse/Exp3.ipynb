{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37d8fab-c38d-46e4-bdde-0475ecf57721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.spatial.distance import cosine\n",
    "from sim_utils import load_examples, Inputexample, CustomTextDataset, freeze_layers, train, test\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b491119-9399-41e1-aad3-5788cdb5ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  2 03:55:10 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:84:00.0 Off |                  N/A |\n",
      "| 24%   27C    P8    11W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:85:00.0 Off |                  N/A |\n",
      "| 22%   28C    P8     1W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  On   | 00000000:88:00.0 Off |                  N/A |\n",
      "| 22%   26C    P8     5W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce ...  On   | 00000000:89:00.0 Off |                  N/A |\n",
      "| 22%   27C    P8     3W / 250W |      1MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d69c6d8-751d-4478-8523-1fe92e0e0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "valid_samples = []\n",
    "valid_labels = []\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "embed_dim = 768\n",
    "batch_size = 16 \n",
    "lr=2e-3  # you can adjust \n",
    "temp = 0.3  # you can adjust \n",
    "lamda = 0.01  # you can adjust  \n",
    "skip_time = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation \n",
    "\n",
    "data_name = 'BANKING77'\n",
    "model_name = 'sup-simcse-roberta-base'\n",
    "layers = 12\n",
    "shot_names = ['train_5','train_10']\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fcd00c-ca91-46ac-84d5-43a2ed3153fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== small train set ====\n",
      "Train on Cross Entropy loss\n",
      "len of dataset : 385\n",
      "===== validation set ====\n",
      "Train on Cross Entropy loss\n",
      "len of dataset : 1540\n",
      "===== test set ====\n",
      "Train on Cross Entropy loss\n",
      "len of dataset : 3080\n",
      "num_class: 77\n",
      "current freezing up to layer : 1\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.3044558735636922, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.33329838622700086, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.333298) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.32277360891366935, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30319470894801154, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.333298--->0.303195) \t Saving The Model\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.31811425097576984, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30214352824471213, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.303195--->0.302144) \t Saving The Model\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.3144958458937608, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.306699385890713, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.3210518911287382, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30523095285737667, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.30804628211182433, \t\t Training acc: 0.025974025949835777\n",
      "train correct :  tensor(10, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3076495650526765, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.3121715397029728, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28911449073197004, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.302144--->0.289114) \t Saving The Model\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.30697618088164885, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2855936356953212, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.289114--->0.285594) \t Saving The Model\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.30127830505371095, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2815584990885351, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.285594--->0.281558) \t Saving The Model\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.30736691487299933, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28390322443726773, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.3096340848253919, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2888112972309063, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.3005427026129388, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28569493789177436, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.3016640142960982, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28383423291243515, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.29568053654262, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2810795811863689, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.281558--->0.281080) \t Saving The Model\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.2986676030344777, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2821448932994496, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.2989471200224641, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2781433155010273, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.281080--->0.278143) \t Saving The Model\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.2915431765766887, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.279172674402014, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.2968076346756576, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27633376462118964, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.278143--->0.276334) \t Saving The Model\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.29321715119597197, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2768538849694388, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.29263686886081447, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27578138809699515, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276334--->0.275781) \t Saving The Model\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.29107964503300654, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2761719155621219, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.28762515179522624, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27572276716108446, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275781--->0.275723) \t Saving The Model\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.29113221973567815, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27478675161089217, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275723--->0.274787) \t Saving The Model\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.2886851063022366, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27500202470011526, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.29097287066571126, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2779029168091811, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.2890758440092012, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27543513465237307, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.28785009755716695, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2745870342502346, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274787--->0.274587) \t Saving The Model\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.2865083112345113, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27421057936433074, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274587--->0.274211) \t Saving The Model\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.286442739313299, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2745083508553443, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.2873422957085944, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.274309060790322, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 40\n",
      "total : 3080\n",
      "current freezing up to layer : 2\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.2996239439233557, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30261791712277897, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.302618) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.31431998834981545, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2952804714054256, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.302618--->0.295280) \t Saving The Model\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.3119574249564827, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3146601636688431, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.30985044256433264, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29508952320396126, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.295280--->0.295090) \t Saving The Model\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.31358769032862277, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2918982651326563, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.295090--->0.291898) \t Saving The Model\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.30899080734748346, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29454205283870943, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.30132764902981846, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29689798107394927, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.30209042066103453, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2843607119151524, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.291898--->0.284361) \t Saving The Model\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.3046089965027648, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2869506377678413, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.299442195892334, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29534862103400294, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.30221502130681815, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28446473765682867, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.2989395079674659, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27953549484153845, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.284361--->0.279535) \t Saving The Model\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.2927328840478674, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2822619933586616, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.29833822126512405, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28106693261629573, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.2952669874414221, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27650216740447203, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279535--->0.276502) \t Saving The Model\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.28904160338562807, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2773593893298855, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.29140346452787325, \t\t Training acc: 0.0181818176060915\n",
      "train correct :  tensor(7, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2781728586593232, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.2893880633564738, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.276200736033452, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276502--->0.276201) \t Saving The Model\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.28791387359817305, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2748579774584089, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276201--->0.274858) \t Saving The Model\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.29170791328727425, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27491086520157854, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.28872621214234984, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2756226226880953, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.2876315141653086, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2750042692407385, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.2885357225096071, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2757894652230399, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.2866798227483576, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2769264490573437, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.2867962936302284, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2768575160534351, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.28800765446254184, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2753046766504065, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.2865296376215947, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27413612898294026, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274858--->0.274136) \t Saving The Model\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.2861592788200874, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2740497412619653, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274136--->0.274050) \t Saving The Model\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.285910938312481, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2740523298065384, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.2864623428939225, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2739569818818724, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274050--->0.273957) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 40\n",
      "total : 3080\n",
      "current freezing up to layer : 3\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.30200219587846233, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3065023456301008, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.306502) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.31475101693884117, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30769121987479076, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.3115222299253786, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29886356483806265, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.306502--->0.298864) \t Saving The Model\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.30169916276807907, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2947521277836391, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.298864--->0.294752) \t Saving The Model\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.3068147374438001, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2943217741978633, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.294752--->0.294322) \t Saving The Model\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.3019208592253846, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3020052881983968, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.3044389619455709, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30106906767015335, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.3082148452857872, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2872760351602133, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.294322--->0.287276) \t Saving The Model\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.30085362583011777, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2890092016814591, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.2991502439820921, \t\t Training acc: 0.0181818176060915\n",
      "train correct :  tensor(7, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29800420643447284, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.3054883226171716, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28297836718621194, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.287276--->0.282978) \t Saving The Model\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.3008511307951692, \t\t Training acc: 0.0181818176060915\n",
      "train correct :  tensor(7, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28018530436924527, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.282978--->0.280185) \t Saving The Model\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.2964048707640016, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2797161346906191, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.280185--->0.279716) \t Saving The Model\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.2988460379761535, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27971316963047177, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279716--->0.279713) \t Saving The Model\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.2990915162222726, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2792814778043078, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279713--->0.279281) \t Saving The Model\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.30077010191880266, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27805022357346176, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279281--->0.278050) \t Saving The Model\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.29427779433015105, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.278549540507329, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.2951683725629534, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2788696347893058, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.29206915892563856, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2770286479553619, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.278050--->0.277029) \t Saving The Model\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.28896026982889544, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2771238302255606, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.29039992419156163, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2753347653847236, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.277029--->0.275335) \t Saving The Model\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.28828440207939643, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27483355751285304, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275335--->0.274834) \t Saving The Model\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.2874975068228585, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27442349954084916, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274834--->0.274423) \t Saving The Model\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.2881891337308017, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27441254838720547, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274423--->0.274413) \t Saving The Model\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.28806667575588474, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2746954577309745, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.28572818087292956, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27419653960636686, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274413--->0.274197) \t Saving The Model\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.2866814737196092, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2739140841868017, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274197--->0.273914) \t Saving The Model\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.28593627756292167, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2738588689209579, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.273914--->0.273859) \t Saving The Model\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.2854949307132077, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27413540784414714, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.2863516758014629, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27401804676303615, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 40\n",
      "total : 3080\n",
      "current freezing up to layer : 4\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.3023376266677658, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28152858343991366, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.281529) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.3066087871402889, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2916038723735066, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.303179863521031, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29908735163800126, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.3067573609290185, \t\t Training acc: 0.0181818176060915\n",
      "train correct :  tensor(7, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2956885978773043, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.30870464374492695, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29025893520999263, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.30353044163097037, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2947189931745653, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.310421795039982, \t\t Training acc: 0.0181818176060915\n",
      "train correct :  tensor(7, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2913811652691333, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.3135076869617809, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30287458029660314, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.30733315975635084, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30051039379912536, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.3045797471876268, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28558888714034836, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.3008492135382318, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2806250309015249, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.281529--->0.280625) \t Saving The Model\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.2959261219222824, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28397292130953306, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.3017695934741528, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28029005094008014, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.280625--->0.280290) \t Saving The Model\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.3077216197917988, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28963036444280055, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.3062287082919827, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28048810432483623, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.2944650513785226, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2836026442515386, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.29620951863078326, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2791420509288837, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.280290--->0.279142) \t Saving The Model\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.29296614659297004, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2782565584430447, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279142--->0.278257) \t Saving The Model\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.2967511573395172, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.275679324199627, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.278257--->0.275679) \t Saving The Model\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.2924065862383161, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2753327877490551, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275679--->0.275333) \t Saving The Model\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.288045343176111, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2761827738254101, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.28897598811558317, \t\t Training acc: 0.020779220387339592\n",
      "train correct :  tensor(8, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27611306178105344, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.2913537855272169, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2756465534111122, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.2886698103570319, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2746491751113495, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275333--->0.274649) \t Saving The Model\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.28825538684795426, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27432048320770264, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274649--->0.274320) \t Saving The Model\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.287290958305458, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27447617704218086, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.2878897146745162, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27432722085482114, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.28723938805716376, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.274169085552166, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274320--->0.274169) \t Saving The Model\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.2856519315149877, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27422439061202014, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.28699355435061763, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27431698867252896, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 40\n",
      "total : 3080\n",
      "current freezing up to layer : 5\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.3010166118671368, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29235451933625456, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.292355) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.3165885590887689, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2958571539296732, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.3056258734170493, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2930485793522426, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.3059878244028463, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30901698477856526, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.312649832143412, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2973752049656657, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.30641213082647945, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2860253186969014, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.292355--->0.286025) \t Saving The Model\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.303328647861233, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29417731854822726, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.30470731660917205, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2897719987027057, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.30397422963922677, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2851704451944921, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.286025--->0.285170) \t Saving The Model\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.30554700331254436, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29535175763167343, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.30180179917967165, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29149965961258134, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.3007456630855412, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27953646337831173, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.285170--->0.279536) \t Saving The Model\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.29887751542128527, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2784115373314201, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279536--->0.278412) \t Saving The Model\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.2986583003750095, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2822590865098037, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.29927281342543566, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2799426790955779, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.2970242438378272, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2768955221423855, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.278412--->0.276896) \t Saving The Model\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.2946429859508168, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2767770720766736, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276896--->0.276777) \t Saving The Model\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.29280082776949, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2797043013882327, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.2990297664295543, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27637922949605176, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276777--->0.276379) \t Saving The Model\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.28968028650655375, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27609817052816416, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276379--->0.276098) \t Saving The Model\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.28918460499156606, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27517249119746223, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276098--->0.275172) \t Saving The Model\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.28743583753511504, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2747823476791382, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275172--->0.274782) \t Saving The Model\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.2885357324179117, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2752802765214598, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.28921858676068196, \t\t Training acc: 0.0181818176060915\n",
      "train correct :  tensor(7, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27486455688228856, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.2874585671858354, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2762377574846342, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.2915330057020311, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27510585258533427, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.28971835173569715, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.275748433695211, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.28645016930320044, \t\t Training acc: 0.020779220387339592\n",
      "train correct :  tensor(8, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27585679865502694, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.2877845194432643, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27580348206804944, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.28816478407228147, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.274296269478736, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274782--->0.274296) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 40\n",
      "total : 3080\n",
      "current freezing up to layer : 6\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.2990073971934133, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3074618992867408, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.307462) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.3149889140934139, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2827120607549494, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.307462--->0.282712) \t Saving The Model\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.3062570943460836, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29220656884181034, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.3113496916634696, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28008401362926927, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.282712--->0.280084) \t Saving The Model\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.30049318709930817, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2992928241754507, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.3078656853019417, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2972507356049178, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.3049556187220982, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2883019834369808, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.2990555243058638, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30828843302540965, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.3006350659704828, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2941978627985174, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.30315764662507294, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2887964149574181, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.2996958769761123, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2827794369164999, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.2979428836277553, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2866991392977826, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.2940953217543565, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2838626607671961, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.30182428731546773, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29905136777208996, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.305468411879106, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28923286710466656, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.30313586197890247, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2850165314488597, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.29859844926115753, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27982747307071437, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.280084--->0.279827) \t Saving The Model\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.30786985545963436, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2927804408135352, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.3029327107714368, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2846380379292872, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.30927122413338004, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.281214942870202, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.3009730054186536, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29558680552940864, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.3009967308539849, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27906315388617575, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279827--->0.279063) \t Saving The Model\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.29572333174866516, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27673648301657144, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279063--->0.276736) \t Saving The Model\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.2977918179004223, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27530566376525084, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276736--->0.275306) \t Saving The Model\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.29368852516273397, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27503815750022986, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275306--->0.275038) \t Saving The Model\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.2956363095865621, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2760284878990867, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.2991806055044199, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2809995273491005, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.29381149713095134, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2763735328401838, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.2906657218933105, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27514323568963384, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.28953144519360036, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2745921110177969, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275038--->0.274592) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 40\n",
      "total : 3080\n",
      "current freezing up to layer : 7\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.3015689874624277, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29172725244001907, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.291727) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.3078009791188426, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3014373286977991, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.30932315479625355, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3119029741782647, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.31255885656777915, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29921518239107997, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.30485421961004083, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29807633982076276, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.3049402819051371, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29361354871229695, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.30512577032114, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29096345746672, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.291727--->0.290963) \t Saving The Model\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.31973078343775363, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2982812869084346, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.314847029648818, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.296039029220482, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.3043559545046323, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28994870774157633, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.290963--->0.289949) \t Saving The Model\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.30619611120843265, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2896983196209003, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.289949--->0.289698) \t Saving The Model\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.30456974970829953, \t\t Training acc: 0.0\n",
      "train correct :  tensor(0, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27953388071679447, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.289698--->0.279534) \t Saving The Model\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.30055800722791004, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2806301699056254, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.2944924515563172, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2824888514233874, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.30011683005791207, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28205096814539526, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.29916891742062257, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2803713346456552, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.29279909505472557, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28266442162649974, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.30353907300280286, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29740572594977044, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.3013439104154512, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2816643919263567, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.2911634977761801, \t\t Training acc: 0.023376623168587685\n",
      "train correct :  tensor(9, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2764761076345072, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.279534--->0.276476) \t Saving The Model\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.2895162359460608, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2759816005632475, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.276476--->0.275982) \t Saving The Model\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.2966772946444425, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2781929270013586, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.2904271125793457, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27544421561352617, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275982--->0.275444) \t Saving The Model\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.28901945337072593, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27525640029411813, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275444--->0.275256) \t Saving The Model\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.2866424585317636, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27491162009053416, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.275256--->0.274912) \t Saving The Model\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.28865727511319245, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2743736527182839, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274912--->0.274374) \t Saving The Model\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.2898569057514141, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27461988678226223, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.287366922799643, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2744916191348782, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.2868365845122895, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.27438173665628807, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.286664224599863, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.274053879217668, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.274374--->0.274054) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct : 40\n",
      "total : 3080\n",
      "current freezing up to layer : 8\n",
      "direct_name : princeton-nlp/sup-simcse-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at princeton-nlp/sup-simcse-roberta-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "classifier.dense.weight\n",
      "classifier.dense.bias\n",
      "classifier.out_proj.weight\n",
      "classifier.out_proj.bias\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.2981415079785632, \t\t Training acc: 0.020779220387339592\n",
      "train correct :  tensor(8, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2880243781325105, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.288024) \t Saving The Model\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.3066849312224946, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.30713708865178097, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.3208446032041079, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.32250139155945223, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.3394184347871062, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3196147225119851, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.33157282618733197, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3380987040408246, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.3508672416984261, \t\t Training acc: 0.012987012974917889\n",
      "train correct :  tensor(5, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3209380391356233, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.3170163315612, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28990580695016044, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.3200654314709948, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3059695274798901, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.3100458578629927, \t\t Training acc: 0.002597402548417449\n",
      "train correct :  tensor(1, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.32614208196664785, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.3142926209932798, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.3048763962535115, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.3174419613627644, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2977860258771228, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.3116352873963195, \t\t Training acc: 0.010389610193669796\n",
      "train correct :  tensor(4, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29320473670959474, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.30748237882341656, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2878665611341402, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.288024--->0.287867) \t Saving The Model\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.30559523198511696, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.29008756318649687, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.2994118591407677, \t\t Training acc: 0.015584414824843407\n",
      "train correct :  tensor(6, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28366503034319196, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.287867--->0.283665) \t Saving The Model\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.2982903344290597, \t\t Training acc: 0.007792207412421703\n",
      "train correct :  tensor(3, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.2825494803391494, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.283665--->0.282549) \t Saving The Model\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.30185353167645346, \t\t Training acc: 0.005194805096834898\n",
      "train correct :  tensor(2, device='cuda:0')\n",
      "train total : 385\n",
      " Validation Loss: 0.28009832029218795, \t\t Validation acc: 0.012987012974917889\n",
      "valid correct :  tensor(20, device='cuda:0')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(0.282549--->0.280098) \t Saving The Model\n"
     ]
    }
   ],
   "source": [
    "## freezing layers\n",
    "for shot_name in shot_names:\n",
    "    \n",
    "    \n",
    "    path_shot = f'../../../{data_name}/{shot_name}/'\n",
    "    valid_path = f'../../../{data_name}/valid/'\n",
    "    test_path = f'../../../{data_name}/test/'\n",
    "\n",
    "\n",
    "    # load data\n",
    "    train_samples = load_examples(path_shot)\n",
    "    valid_samples = load_examples(valid_path)\n",
    "    test_samples = load_examples(test_path)\n",
    "\n",
    "\n",
    "    print(\"===== small train set ====\")\n",
    "\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(train_samples)):\n",
    "        data.append(train_samples[i].text)\n",
    "        labels.append(train_samples[i].label)\n",
    "\n",
    "\n",
    "    train_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=False)\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"===== validation set ====\")\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(valid_samples)):\n",
    "        data.append(valid_samples[i].text)\n",
    "        labels.append(valid_samples[i].label)\n",
    "\n",
    "    valid_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=False)\n",
    "    valid_loader = DataLoader(valid_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"===== test set ====\")\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(test_samples)):\n",
    "        data.append(test_samples[i].text)\n",
    "        labels.append(test_samples[i].label)\n",
    "\n",
    "    test_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=False)\n",
    "    test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "     # got the number of unique classes from dataset\n",
    "    num_class = len(np.unique(np.array(labels)))\n",
    "\n",
    "     # get text label of uniqure classes\n",
    "    unique_label = np.unique(np.array(labels))\n",
    "\n",
    "     # map text label to index classes\n",
    "    label_maps = {unique_label[i]: i for i in range(len(unique_label))}\n",
    "\n",
    "    \n",
    "    print(\"num_class:\",num_class)\n",
    "    \n",
    "\n",
    "    lines = ['test acc']\n",
    "    \n",
    "    for cur_layer in range(12):\n",
    "            \n",
    "            print(\"current freezing up to layer :\",cur_layer+1)\n",
    "            exp_name = f'{model_name}_lr={lr}_t={temp}_{data_name}_{shot_name}_{cur_layer+1}'\n",
    "            direct_name = f\"princeton-nlp/{model_name}\"\n",
    "\n",
    "            print(\"direct_name :\",direct_name)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(direct_name)\n",
    "            config = AutoConfig.from_pretrained(direct_name)\n",
    "            config.num_labels = num_class\n",
    "            simcse = AutoModelForSequenceClassification.from_pretrained(direct_name,config=config)\n",
    "             \n",
    "            simcse = freeze_layers(simcse,freeze_layers_count=cur_layer+1)\n",
    "            \n",
    "            optimizer= AdamW(simcse.parameters(), lr=lr)\n",
    "            simcse = simcse.to(device)\n",
    "\n",
    "            train_log, valid_log = train(exp_name,simcse,device,label_maps,optimizer,train_loader,valid_loader,train_data,valid_data,tokenizer,epochs=30)\n",
    "\n",
    "            \n",
    "            PATH = f'../../../fewshot_models/{exp_name}.pth'\n",
    "            best_model = AutoModelForSequenceClassification.from_pretrained(direct_name,config=config)\n",
    "            # Model class must be defined somewhere\n",
    "            best_model.load_state_dict(torch.load(PATH))\n",
    "            best_model = best_model.to(device)\n",
    "\n",
    "\n",
    "            test_acc = test(best_model,device,label_maps,test_loader,len(test_data),tokenizer)\n",
    "\n",
    "            test_acc = 100 * test_acc\n",
    "                  \n",
    "\n",
    "            res = f'data_name:{data_name}_model:{model_name}_{shot_name}_{cur_layer+1}_test_acc:{str(test_acc)}'\n",
    "            lines.append(res)\n",
    "\n",
    "\n",
    "    with open(f'exp3_result_{data_name}_{shot_name}.txt', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e733083-975d-444d-970e-e1570579649d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
