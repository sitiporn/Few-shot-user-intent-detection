{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9067b45e-7fe7-4f46-a833-2819e2120ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.spatial.distance import cosine\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "#comment this if you are not using puffer\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cbbb405-efb2-4c1b-81fb-d7056b023e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(file_path, do_lower_case=True):\n",
    "    examples = []\n",
    "    \n",
    "    with open('{}/seq.in'.format(file_path),'r',encoding=\"utf-8\") as f_text, open('{}/label'.format(file_path),'r',encoding=\"utf-8\") as f_label:\n",
    "        for text, label in zip(f_text, f_label):\n",
    "            \n",
    "            e = Inputexample(text.strip(),label=label.strip())\n",
    "            examples.append(e)\n",
    "            \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf32e525-9e33-4e8f-b8ee-95564201df87",
   "metadata": {},
   "source": [
    "## Each sample has a sentence and label format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed774baf-945e-4fa9-8b52-621ea34a5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inputexample(object):\n",
    "    def __init__(self,text_a,label = None):\n",
    "        self.text = text_a\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49eb741e-7cf4-4432-97ef-87a0c3a91fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom dataset class\n",
    "# ===  =  Hint =  ===\n",
    "# can train on two condition \n",
    "# 1.) trainig training with supervise contrastive loss and cross entropy loss using in question 5.) \n",
    "#    when self.repeated_label == True:\n",
    "# 2.) train only cross entropy loss use in question 4.)\n",
    "#    when self.repeated_label == False:\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self,labels,text,batch_size,repeated_label:bool=False):\n",
    "        self.labels = labels\n",
    "        self.text = text\n",
    "        self.batch_size = batch_size \n",
    "        self.count = 0 \n",
    "        self.batch_labels = []\n",
    "        self.repeated_label = repeated_label\n",
    "        \n",
    "        if self.repeated_label == True:\n",
    "            print(\"Train on Combine between Supervised Contrastive and Cross Entropy loss\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Train on Cross Entropy loss\")\n",
    "            \n",
    "        \n",
    "        print(\"len of dataset :\",len(self.labels))\n",
    "              \n",
    "     \n",
    "          \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # write code here for 1)\n",
    "        if self.repeated_label == True:\n",
    "        \n",
    "            if len(np.unique(self.batch_labels)) == self.batch_size - 1:\n",
    "\n",
    "\n",
    "                while True:\n",
    "                    idx = np.random.choice(len(self.labels))\n",
    "\n",
    "                    if self.labels[idx]  in self.batch_labels:\n",
    "\n",
    "                       \n",
    "                        break\n",
    "\n",
    "        self.batch_labels.append(self.labels[idx])\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        data = self.text[idx]\n",
    "        \n",
    "        sample = {\"Class\": label,\"Text\": data}\n",
    "\n",
    "\n",
    "    \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf1ba9-e632-45d5-ad6b-4a80af02490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d5c23c-b9d0-4f91-8e17-6e1752b0f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model,freeze_layers_count:int=0):\n",
    "\n",
    "        \"\"\"\n",
    "        model : model object that we create \n",
    "        freeze_layers_count : the number of layers to freeze \n",
    "        \"\"\"\n",
    "        # write the code here\n",
    "    \n",
    "        # should not more than the number of layers in a backbone\n",
    "        assert freeze_layers_count <= 12\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "           \n",
    "\n",
    "            keys = name.split(\".\")\n",
    "\n",
    "            if str(freeze_layers_count) in keys or 'classifier' in keys:\n",
    "                break\n",
    "            \n",
    "            param.requires_grad = False \n",
    "\n",
    "\n",
    "        #print all parameter that we want to train from scratch \n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            \n",
    "            if param.requires_grad == True:\n",
    "                 \n",
    "                print(name)\n",
    "        \n",
    "    \n",
    "        return model     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26842bcf-7641-41f6-ab02-89bd30c01e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,train_loader,valid_loader,epochs:int=30):\n",
    "\n",
    "    train_loss_hist = [] \n",
    "    valid_loss_hist = []\n",
    "    \n",
    "    train_acc_hist = []\n",
    "    valid_acc_hist = []\n",
    "    \n",
    "\n",
    "    test_acc = []\n",
    "\n",
    "    min_valid_loss = np.inf\n",
    "   \n",
    "    for e in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "       \n",
    "        model.train()\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "       \n",
    "    \n",
    "        for (idx, batch) in enumerate(train_loader):\n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "            #assert len(np.unique(batch[\"Class\"])) < len(batch[\"Class\"])  \n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "             # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Foward pass \n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "            # get loss and output \n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            \n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "            \n",
    "           # Update Weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate nums of correction \n",
    "            correct += (torch.max(logits,dim=-1)[1] == labels).sum()\n",
    "            \n",
    "           \n",
    "            \n",
    "            #clear_output(wait=True)\n",
    "        \n",
    "        train_loss_hist.append(running_loss/len(train_data))\n",
    "        train_acc_hist.append(correct/len(train_data))\n",
    "        \n",
    "        \n",
    "        print(f'======  Epoch {e+1} ====== ')\n",
    "        print(f' Training Loss: {running_loss/len(train_data)}, \\t\\t Training acc: {correct/len(train_data)}')\n",
    "        \n",
    "        print(\"train correct : \",correct)\n",
    "        print(\"train total :\",len(train_data))\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        model.eval()     # Optional when not using Model Specific layer\n",
    "        log_correct = []\n",
    "        \n",
    "        \n",
    "        for (idx, batch) in enumerate(valid_loader):\n",
    "\n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Foward pass \n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "            \n",
    "            # get loss and output \n",
    "            loss, logits = outputs[:2]\n",
    "        \n",
    "            \n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate nums of correction \n",
    "            correct += (torch.max(logits,dim=-1)[1] == labels).sum()\n",
    "            \n",
    "           \n",
    "        #  add to collect log \n",
    "        \n",
    "        valid_loss_hist.append(running_loss/len(valid_data))\n",
    "        valid_acc_hist.append(correct/len(valid_data))\n",
    "        \n",
    "        print(f' Validation Loss: {running_loss/len(valid_data)}, \\t\\t Validation acc: {correct/len(valid_data)}')\n",
    "        \n",
    "        print(\"valid correct : \",correct)\n",
    "        print(\"valid total :\",len(valid_data))\n",
    "       \n",
    "        # save best current model \n",
    "        if min_valid_loss > (running_loss/len(valid_data)):\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{running_loss/len(valid_data):.6f}) \\t Saving The Model')\n",
    "            min_valid_loss = running_loss/len(valid_data) \n",
    "            torch.save(model.state_dict(), 'saved_model.pth')\n",
    "            \n",
    "           \n",
    "    return (train_acc_hist, train_loss_hist), (valid_acc_hist, valid_loss_hist)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e365ee63-74ce-421f-964b-b644b9fdc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "valid_samples = []\n",
    "valid_labels = []\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "embed_dim = 768\n",
    "batch_size = 4 \n",
    "lr=2e-3  # you can adjust \n",
    "temp = 0.3  # you can adjust \n",
    "lamda = 0.01  # you can adjust  \n",
    "skip_time = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation \n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3f507e-cb1d-4f05-baeb-89871d78a068",
   "metadata": {},
   "source": [
    "### The Aim of these training is to fine tuning on few shot setting on text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf4a20-f3c3-4725-928e-2ddb847daea8",
   "metadata": {},
   "source": [
    "Path example of train, validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58a34c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_5shot = f'../../../../Thesis/CLINC150/train_5/'\n",
    "valid_path = f'../../../../Thesis/CLINC150/valid/'\n",
    "test_path = f'../../../../Thesis/CLINC150//test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b4755c-3204-4e26-98e7-de46077b84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our models. The package will take care of downloading the models automatically\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc0b272e-288d-45da-bd17-438ffd3b959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== small train set ====\n",
      "Train on Cross Entropy loss\n",
      "len of dataset : 750\n",
      "===== validation set ====\n",
      "Train on Cross Entropy loss\n",
      "len of dataset : 3000\n",
      "===== test set ====\n",
      "Train on Cross Entropy loss\n",
      "len of dataset : 4500\n"
     ]
    }
   ],
   "source": [
    "# Download data fewshot \n",
    "# https://downgit.github.io/#/home?url=https:%2F%2Fgithub.com%2Fjianguoz%2FFew-Shot-Intent-Detection%2Ftree%2Fmain%2FDatasets%2FHWU64\n",
    "\n",
    "# load data\n",
    "train_samples = load_examples(path_5shot)\n",
    "valid_samples = load_examples(valid_path)\n",
    "test_samples = load_examples(test_path)\n",
    "\n",
    "\n",
    "print(\"===== small train set ====\")\n",
    "\n",
    "for i in range(len(train_samples)):\n",
    "    data.append(train_samples[i].text)\n",
    "    labels.append(train_samples[i].label)\n",
    "\n",
    "\n",
    "train_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=False)\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== validation set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(valid_samples)):\n",
    "    data.append(valid_samples[i].text)\n",
    "    labels.append(valid_samples[i].label)\n",
    "\n",
    "valid_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=False)\n",
    "valid_loader = DataLoader(valid_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== test set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "    \n",
    "for i in range(len(test_samples)):\n",
    "    data.append(test_samples[i].text)\n",
    "    labels.append(test_samples[i].label)\n",
    "\n",
    "test_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=False)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# got the number of unique classes from dataset\n",
    "num_class = len(np.unique(np.array(labels)))\n",
    "\n",
    "# get text label of uniqure classes\n",
    "unique_label = np.unique(np.array(labels))\n",
    "\n",
    "# map text label to index classes\n",
    "label_maps = {unique_label[i]: i for i in range(len(unique_label))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3521713-11f4-4b02-bc2a-7e23f50a877e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.state_dict()['bert.embeddings.word_embeddings.weight'].data\n",
    "\n",
    "torch.all(x== model.state_dict()['bert.embeddings.word_embeddings.weight'].data)\n",
    "# torch.rand(model.state_dict()['bert.embeddings.word_embeddings.weight'].data.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bffa36c1-624d-43c1-94a6-a1e4f3ea730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59017e63-2a32-4156-ab50-93be0cbe5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8a8c263-984d-476c-a1f4-a17cabe5dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "simcse = AutoModel.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ba5c716-7bf0-481d-8786-88ad2af2c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "config.num_labels = 150\n",
    "model = AutoModelForSequenceClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f06d9-ee22-4f09-9070-5e693a9f01ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6dcdd28d-d203-40bd-a684-02289964b843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with feed old weight to new architecure\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for new_name, new_param in model.named_parameters():\n",
    "\n",
    "        new_keys = new_name.split(\".\")\n",
    "\n",
    "        for old_name, old_param in simcse.named_parameters():\n",
    "\n",
    "            old_keys = old_name.split(\".\")\n",
    "\n",
    "            if new_keys[1:] == old_keys:\n",
    "                # print(\"-----------------------\")\n",
    "                # print(new_keys[1:])\n",
    "                # print(old_keys)\n",
    "                # print(\"-----------------------\")\n",
    "                # if new_param.shape == old_param.shape\n",
    "                # feed old weight to new network\n",
    "                model.state_dict()[new_name].data.copy_(old_param)\n",
    "                \n",
    "                assert torch.all(model.state_dict()[new_name] == simcse.state_dict()[old_name])\n",
    "                                \n",
    "print(\"Done with feed old weight to new architecure\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "36aceb20-a97a-4945-abba-67bac08c5c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/st121532/work/Thesis/Few-shot-user-intent-detection/Few-shot-user-intent-detection/simcse\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6153750c-5a77-45b9-8480-881ea775a8c5",
   "metadata": {},
   "source": [
    "model = freeze_layers(model,freeze_layers_count=12)\n",
    "model.config.num_labels = 150\n",
    "# Using adam optimizer \n",
    "optimizer= AdamW(model.parameters(), lr=lr)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de0011ad-62d5-4d80-9f9a-4434eb746694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_log, valid_log = train(model,optimizer,train_loader,valid_loader,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab13ab-eaa0-455d-89ec-94e5358b6f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
