{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31687e10-7a63-4592-94a3-dae140324863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.spatial.distance import cosine\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'simcse')))\n",
    "from sim_utils import load_examples, Inputexample, CustomTextDataset, freeze_layers, test\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, AutoModel, AutoTokenizer\n",
    "from loss import Similarity, create_supervised_pair, supervised_contrasive_loss\n",
    "\n",
    " \n",
    "#comment this if you are not using puffer\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2f966-fc6a-479f-bf71-e6bfe8b37743",
   "metadata": {},
   "source": [
    "### Train With Combine loss between Cross Entropy and SuperVised Contrastive loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96023b6-4a7f-4f25-b175-bf5cdb1136a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f656717-e0e7-4e51-8541-6ab4c2b25c59",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bd1ea-56ff-4e4c-a2ef-36787f04c2d1",
   "metadata": {},
   "source": [
    "### The Aim of these training is to fine tuning on few shot setting on text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361f0ce-c287-4cb2-9ebd-d8487c3f35bc",
   "metadata": {},
   "source": [
    "Path example of train, validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e365ee63-74ce-421f-964b-b644b9fdc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "valid_samples = []\n",
    "valid_labels = []\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "embed_dim = 768\n",
    "batch_size = 16 \n",
    "lr=2e-3  # you can adjust \n",
    "temp = 0.3  # you can adjust \n",
    "lamda = 0.01  # you can adjust  \n",
    "skip_time = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation \n",
    "data_name = 'BANKING77'\n",
    "model_name = 'simcse_sup'\n",
    "shot_name = 'train_5'\n",
    "exp_name = f'{model_name}_lr={lr}_t={temp}_{data_name}_{shot_name}'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e58a34c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train path :  ../../../../Thesis/BANKING77/train_5/\n",
      "valid path :  ../../../../Thesis/BANKING77/valid/\n",
      "test path  :  ../../../../Thesis/BANKING77/test/\n",
      "experiment code name : simcse_sup_lr=0.002_t=0.3_BANKING77_train_5\n"
     ]
    }
   ],
   "source": [
    "path_shot = f'../../../../Thesis/{data_name}/{shot_name}/'\n",
    "valid_path = f'../../../../Thesis/{data_name}/valid/'\n",
    "test_path = f'../../../../Thesis/{data_name}/test/'\n",
    "print(\"train path : \",path_shot)\n",
    "print(\"valid path : \",valid_path)\n",
    "print(\"test path  : \",test_path)\n",
    "print(\"experiment code name :\",exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24b0c99-e4fe-4be3-a9dc-15d707ee4be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== small train set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 385\n",
      "===== validation set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 1540\n",
      "===== test set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 3080\n"
     ]
    }
   ],
   "source": [
    "# Download data fewshot \n",
    "# https://downgit.github.io/#/home?url=https:%2F%2Fgithub.com%2Fjianguoz%2FFew-Shot-Intent-Detection%2Ftree%2Fmain%2FDatasets%2FHWU64\n",
    "\n",
    "# load data\n",
    "train_samples = load_examples(path_shot)\n",
    "valid_samples = load_examples(valid_path)\n",
    "test_samples = load_examples(test_path)\n",
    "\n",
    "\n",
    "print(\"===== small train set ====\")\n",
    "\n",
    "for i in range(len(train_samples)):\n",
    "    data.append(train_samples[i].text)\n",
    "    labels.append(train_samples[i].label)\n",
    "\n",
    "\n",
    "train_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== validation set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(valid_samples)):\n",
    "    data.append(valid_samples[i].text)\n",
    "    labels.append(valid_samples[i].label)\n",
    "\n",
    "valid_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "valid_loader = DataLoader(valid_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "print(\"===== test set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "    \n",
    "for i in range(len(test_samples)):\n",
    "    data.append(test_samples[i].text)\n",
    "    labels.append(test_samples[i].label)\n",
    "\n",
    "test_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# got the number of unique classes from dataset\n",
    "num_class = len(np.unique(np.array(labels)))\n",
    "\n",
    "# get text label of uniqure classes\n",
    "unique_label = np.unique(np.array(labels))\n",
    "\n",
    "# map text label to index classes\n",
    "label_maps = {unique_label[i]: i for i in range(len(unique_label))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe650eb-d9fc-443c-b766-f6a35ccc78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "config = AutoConfig.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "config.num_labels = num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8802747d-5070-48ff-beb2-83ec7a2dfadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at princeton-nlp/sup-simcse-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "simcse = AutoModelForSequenceClassification.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\",config=config)\n",
    "simcse = freeze_layers(simcse,freeze_layers_count=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a05b5d9c-ba56-40bb-bce3-74f156a26462",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= AdamW(simcse.parameters(), lr=lr)\n",
    "simcse = simcse.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce7446a5-a283-4a58-a327-daa46974d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive_learnig(model,optimizer,loss_fct,label_maps,temp,train_loader,tokenizer,valid_loader,train_data,valid_data,device,lamda,epochs:int=30):\n",
    "    \n",
    "    \n",
    "    train_loss_hist = [] \n",
    "    valid_loss_hist = []\n",
    "    \n",
    "    train_acc_hist = []\n",
    "    valid_acc_hist = []\n",
    "    \n",
    "\n",
    "    test_acc = []\n",
    "\n",
    "    min_valid_loss = np.inf\n",
    "    skip_train = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation\n",
    "    skip_valid = 0 \n",
    "    for e in range(epochs):  # loop over the dataset multiple times\n",
    " \n",
    "        model.train()\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "       \n",
    "\n",
    "        for (idx, batch) in enumerate(train_loader):\n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "            \n",
    "          \n",
    "     \n",
    "            \n",
    "            if len(np.unique(batch[\"Class\"])) == len(batch[\"Class\"]):\n",
    "                skip_train +=1 \n",
    "                print(\"skip_train:\",skip_train)\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # move parameter to device\n",
    "            # inputs decompose into  input_ids, token_type_ids, attention_mask\n",
    "\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "             # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(**inputs,labels=labels,output_hidden_states=True)     \n",
    "        \n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            last_hidden_states = hidden_states[12]\n",
    "\n",
    "            # https://stackoverflow.com/questions/63040954/how-to-extract-and-use-bert-encodings-of-sentences-for-text-similarity-among-sen \n",
    "            # (batch_size,seq_len,embed_dim)\n",
    "            h = last_hidden_states[:,0,:]\n",
    "\n",
    "            \n",
    "            # create pair samples\n",
    "            T, h_i, h_j, idx_yij = create_supervised_pair(h,batch['Class'],debug=False)\n",
    "\n",
    "             \n",
    "            # if h_i is None:\n",
    "            #     print(\"train : h_i is skipped:\")\n",
    "            #     print(\"labels :\",batch['Class'])\n",
    "            #     print(\"unique labels :\",len(np.unique(batch['Class'])))\n",
    "            #     print(\"full labels :\",len(batch['Class']))\n",
    "            #     continue\n",
    "\n",
    "            # supervised contrastive loss \n",
    "            \n",
    "            loss_s_cl = supervised_contrasive_loss(device,loss_fct,h_i, h_j, h, T,temp=temp,idx_yij=idx_yij,debug=False)\n",
    "\n",
    "            # cross entropy loss\n",
    "            loss_classify, logits = outputs[:2]\n",
    "\n",
    "            # loss total\n",
    "            loss = loss_s_cl + (lamda * loss_classify )\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate nums of correction \n",
    "            correct += (torch.max(logits,dim=-1)[1] == labels).sum()\n",
    "\n",
    "        \n",
    "        train_loss_hist.append(running_loss/len(train_data))\n",
    "        train_acc_hist.append(correct/len(train_data))\n",
    "        \n",
    "        \n",
    "        print(f'======  Epoch {e+1} ====== ')\n",
    "        print(f' Training Loss: {running_loss/len(train_data)}, \\t\\t Training acc: {correct/len(train_data)}')\n",
    "        \n",
    "        print(\"train correct : \",correct)\n",
    "        print(\"train total :\",len(train_data))\n",
    "        \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        model.eval()     # Optional when not using Model Specific layer\n",
    "        log_correct = []\n",
    "        \n",
    "        \n",
    "        for (idx, batch) in enumerate(valid_loader):\n",
    "            \n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "            \n",
    "            \n",
    "              \n",
    "            if len(np.unique(batch[\"Class\"])) == len(batch[\"Class\"]):\n",
    "                skip_valid +=1 \n",
    "                print(\"skip_valid\",skip_valid)\n",
    "                continue\n",
    "\n",
    "            \n",
    "            #assert len(np.unique(batch[\"Class\"])) < len(batch[\"Class\"])  \n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "             # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = model(**inputs,labels=labels,output_hidden_states=True)     \n",
    "        \n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            last_hidden_states = hidden_states[12]\n",
    "\n",
    "            # https://stackoverflow.com/questions/63040954/how-to-extract-and-use-bert-encodings-of-sentences-for-text-similarity-among-sen \n",
    "            # (batch_size,seq_len,embed_dim)\n",
    "            h = last_hidden_states[:,0,:]\n",
    "\n",
    "            # create pair samples\n",
    "            T, h_i, h_j, idx_yij = create_supervised_pair(h,batch['Class'],debug=False)\n",
    "\n",
    "#             if h_i is None:\n",
    "#                 print(\"valid : h_i is skipped:\")\n",
    "#                 print(\"labels :\",batch['Class'])\n",
    "#                 print(\"unique labels :\",len(np.unique(batch['Class'])))\n",
    "#                 print(\"full labels :\",len(batch['Class']))\n",
    "#                 continue\n",
    "          \n",
    "            # supervised contrastive loss \n",
    "            loss_s_cl = supervised_contrasive_loss(device,loss_fct,h_i, h_j, h, T,temp=temp,idx_yij=idx_yij,debug=False)\n",
    "\n",
    "            # cross entropy loss\n",
    "            loss_classify, logits = outputs[:2]\n",
    "\n",
    "            # loss total\n",
    "            loss = loss_s_cl + (lamda * loss_classify )\n",
    "            \n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate nums of correction \n",
    "            correct += (torch.max(logits,dim=-1)[1] == labels).sum()\n",
    "            \n",
    "        # add code to logging\n",
    "        valid_loss_hist.append(running_loss/len(valid_data))\n",
    "        valid_acc_hist.append(correct/len(valid_data))\n",
    "        \n",
    "        print(f' Validation Loss: {running_loss/len(valid_data)}, \\t\\t Validation acc: {correct/len(valid_data)}')\n",
    "        \n",
    "        print(\"valid correct : \",correct)\n",
    "        print(\"valid total :\",len(valid_data))\n",
    "       \n",
    "        \n",
    "        \n",
    "        # save best current model \n",
    "        if min_valid_loss > (running_loss/len(valid_data)):\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{running_loss/len(valid_data):.6f}) \\t Saving The Model')\n",
    "            min_valid_loss = running_loss/len(valid_data) \n",
    "            torch.save(model.state_dict(), 'saved_model.pth')\n",
    "            \n",
    "       \n",
    "            \n",
    "    return (train_acc_hist, train_loss_hist), (valid_acc_hist, valid_loss_hist)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5354b9e1-cf06-499c-bb39-4f8dd5882b33",
   "metadata": {},
   "source": [
    "#What is fundamental problem that When combine model get stuck\n",
    "\n",
    "# Fine-tunning problem\n",
    "  1. ev -> model cannot converge with this loss\n",
    "  2. con -> something wrong in fine-tune stage\n",
    "\n",
    "# Pretrain Problem\n",
    "  1. some are get proof which not sure yet \n",
    "      * check in material \n",
    "  2. cannot make con \n",
    "      * if you use pretrained model and fine-tune on Cross Entropy  and the result are accepted means \n",
    "      the main problem that we face before the root cause come from stage2 segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22eb1698-2e4c-46a2-8d34-7a4f3ea70965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "405c316b-3cf0-4cd5-acdd-ec7879508ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip_train: 1\n",
      "======  Epoch 1 ====== \n",
      " Training Loss: 0.07396520763248592, \t\t Training acc: 0.9948051571846008\n",
      "train correct :  tensor(383, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 1\n",
      " Validation Loss: 0.07854061784682335, \t\t Validation acc: 0.1389610320329666\n",
      "valid correct :  tensor(214, device='cuda:1')\n",
      "valid total : 1540\n",
      "Validation Loss Decreased(inf--->0.078541) \t Saving The Model\n",
      "skip_train: 2\n",
      "======  Epoch 2 ====== \n",
      " Training Loss: 0.07359591954714292, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 2\n",
      " Validation Loss: 0.07856871207039078, \t\t Validation acc: 0.13311688601970673\n",
      "valid correct :  tensor(205, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 3\n",
      "======  Epoch 3 ====== \n",
      " Training Loss: 0.07372118312043029, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.07950021396209667, \t\t Validation acc: 0.1389610320329666\n",
      "valid correct :  tensor(214, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 4\n",
      "======  Epoch 4 ====== \n",
      " Training Loss: 0.07382460910004454, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 3\n",
      " Validation Loss: 0.07886443548388296, \t\t Validation acc: 0.11103896051645279\n",
      "valid correct :  tensor(171, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 5\n",
      "======  Epoch 5 ====== \n",
      " Training Loss: 0.07377160648246864, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 4\n",
      " Validation Loss: 0.07875147848934322, \t\t Validation acc: 0.13181817531585693\n",
      "valid correct :  tensor(203, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 6\n",
      "======  Epoch 6 ====== \n",
      " Training Loss: 0.07374906044501763, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 5\n",
      " Validation Loss: 0.07885643112195002, \t\t Validation acc: 0.13181817531585693\n",
      "valid correct :  tensor(203, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 7\n",
      "======  Epoch 7 ====== \n",
      " Training Loss: 0.07351564927534623, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.07968104373325001, \t\t Validation acc: 0.11623376607894897\n",
      "valid correct :  tensor(179, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 8\n",
      "======  Epoch 8 ====== \n",
      " Training Loss: 0.0737331941530302, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.07964482439028753, \t\t Validation acc: 0.12467531859874725\n",
      "valid correct :  tensor(192, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 9\n",
      "======  Epoch 9 ====== \n",
      " Training Loss: 0.07409400692233792, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.0795931472793802, \t\t Validation acc: 0.13051947951316833\n",
      "valid correct :  tensor(201, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 10\n",
      "======  Epoch 10 ====== \n",
      " Training Loss: 0.07375592690009575, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 6\n",
      " Validation Loss: 0.0790982283555068, \t\t Validation acc: 0.12532466650009155\n",
      "valid correct :  tensor(193, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 11\n",
      "======  Epoch 11 ====== \n",
      " Training Loss: 0.07339487447367087, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 7\n",
      " Validation Loss: 0.07895542189672396, \t\t Validation acc: 0.13246752321720123\n",
      "valid correct :  tensor(204, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 12\n",
      "======  Epoch 12 ====== \n",
      " Training Loss: 0.07391051472007454, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 8\n",
      " Validation Loss: 0.07895853248509493, \t\t Validation acc: 0.12857142090797424\n",
      "valid correct :  tensor(198, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 13\n",
      "======  Epoch 13 ====== \n",
      " Training Loss: 0.07383365166651738, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.07973852064702418, \t\t Validation acc: 0.12532466650009155\n",
      "valid correct :  tensor(193, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 14\n",
      "======  Epoch 14 ====== \n",
      " Training Loss: 0.07363652656604718, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 9\n",
      " Validation Loss: 0.07910951809449629, \t\t Validation acc: 0.11753246188163757\n",
      "valid correct :  tensor(181, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 15\n",
      "======  Epoch 15 ====== \n",
      " Training Loss: 0.07400977549614844, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 10\n",
      " Validation Loss: 0.07903213748684178, \t\t Validation acc: 0.12727272510528564\n",
      "valid correct :  tensor(196, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 16\n",
      "======  Epoch 16 ====== \n",
      " Training Loss: 0.07357551649019316, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.07983684439163703, \t\t Validation acc: 0.12662337720394135\n",
      "valid correct :  tensor(195, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 17\n",
      "======  Epoch 17 ====== \n",
      " Training Loss: 0.07371658226112267, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 11\n",
      " Validation Loss: 0.07904344340423484, \t\t Validation acc: 0.12077921628952026\n",
      "valid correct :  tensor(186, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 18\n",
      "======  Epoch 18 ====== \n",
      " Training Loss: 0.07389641984716638, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 12\n",
      " Validation Loss: 0.07915354123363247, \t\t Validation acc: 0.12467531859874725\n",
      "valid correct :  tensor(192, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 19\n",
      "======  Epoch 19 ====== \n",
      " Training Loss: 0.07323367967234029, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 13\n",
      " Validation Loss: 0.07926273075017062, \t\t Validation acc: 0.11168830841779709\n",
      "valid correct :  tensor(172, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 20\n",
      "======  Epoch 20 ====== \n",
      " Training Loss: 0.0736834885238053, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.07993755657951553, \t\t Validation acc: 0.11688311398029327\n",
      "valid correct :  tensor(180, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 21\n",
      "======  Epoch 21 ====== \n",
      " Training Loss: 0.0739863819890208, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 14\n",
      " Validation Loss: 0.0793172030479877, \t\t Validation acc: 0.1071428582072258\n",
      "valid correct :  tensor(165, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 22\n",
      "======  Epoch 22 ====== \n",
      " Training Loss: 0.07395192028640153, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 15\n",
      " Validation Loss: 0.07913151042801993, \t\t Validation acc: 0.13636364042758942\n",
      "valid correct :  tensor(210, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 23\n",
      "======  Epoch 23 ====== \n",
      " Training Loss: 0.07376499361806102, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.08002356244372083, \t\t Validation acc: 0.13051947951316833\n",
      "valid correct :  tensor(201, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 24\n",
      "======  Epoch 24 ====== \n",
      " Training Loss: 0.07386823226879169, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.08016204864947826, \t\t Validation acc: 0.11428571492433548\n",
      "valid correct :  tensor(176, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 25\n",
      "======  Epoch 25 ====== \n",
      " Training Loss: 0.0737992373379794, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 16\n",
      " Validation Loss: 0.07936531413685191, \t\t Validation acc: 0.1077922061085701\n",
      "valid correct :  tensor(166, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 26\n",
      "======  Epoch 26 ====== \n",
      " Training Loss: 0.07390135944663705, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 17\n",
      " Validation Loss: 0.07926871451464566, \t\t Validation acc: 0.12987013161182404\n",
      "valid correct :  tensor(200, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 27\n",
      "======  Epoch 27 ====== \n",
      " Training Loss: 0.0736703482541171, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 18\n",
      " Validation Loss: 0.07930682758232216, \t\t Validation acc: 0.11688311398029327\n",
      "valid correct :  tensor(180, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 28\n",
      "======  Epoch 28 ====== \n",
      " Training Loss: 0.07384499234038513, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.07996574555124555, \t\t Validation acc: 0.12402597069740295\n",
      "valid correct :  tensor(191, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 29\n",
      "======  Epoch 29 ====== \n",
      " Training Loss: 0.07398447897527125, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      " Validation Loss: 0.08018969312890784, \t\t Validation acc: 0.12532466650009155\n",
      "valid correct :  tensor(193, device='cuda:1')\n",
      "valid total : 1540\n",
      "skip_train: 30\n",
      "======  Epoch 30 ====== \n",
      " Training Loss: 0.07399335464873871, \t\t Training acc: 0.997402548789978\n",
      "train correct :  tensor(384, device='cuda:1')\n",
      "train total : 385\n",
      "skip_valid 19\n",
      " Validation Loss: 0.07929912859743292, \t\t Validation acc: 0.12857142090797424\n",
      "valid correct :  tensor(198, device='cuda:1')\n",
      "valid total : 1540\n"
     ]
    }
   ],
   "source": [
    "train_log, valid_log = train_contrastive_learnig(simcse,optimizer,nn.CrossEntropyLoss(),label_maps,temp,train_loader,tokenizer,valid_loader,train_data,valid_data,device,lamda=lamda,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig.suptitle('Show t 4.1 by freezing 9 layers')\n",
    "ax1.plot(torch.tensor(train_log[0] , device = 'cpu').tolist())\n",
    "ax1.plot(torch.tensor(valid_log[0] , device = 'cpu').tolist())\n",
    "ax1.legend(['train_acc','validation_acc'])\n",
    "ax2.plot(torch.tensor(train_log[1] , device = 'cpu').tolist())\n",
    "ax2.plot(torch.tensor(valid_log[1] , device = 'cpu').tolist())\n",
    "ax2.legend(['train_loss','validation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02deb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(simcse,device,label_maps,test_loader,len(test_data),tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2771a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy : {100 * test_acc} %') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5264623-e494-4b7a-9c3d-ca1d58d9b797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
