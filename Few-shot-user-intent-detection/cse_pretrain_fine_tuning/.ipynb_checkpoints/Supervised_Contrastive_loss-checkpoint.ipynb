{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b968508-17cd-4e48-94b7-8cbe5b5c3153",
   "metadata": {},
   "source": [
    "### Please read papers as following \n",
    "- SUPERVISED CONTRASTIVE LEARNING FOR PRE-TRAINED LANGUAGE MODEL FINE-TUNING\n",
    "  - link: https://openreview.net/pdf?id=cu7IUiO\n",
    "- Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning\n",
    "  - link: https://arxiv.org/abs/2109.06349"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e9183-c1e1-40e4-be4d-6d74fb02ece4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cross Entropy loss\n",
    "$$\n",
    "\\mathcal{L}_\\text{CE} =-\\frac{1}{m} \\sum_{i=1}^{m} yi \\cdot log(\\hat{yi})\n",
    "$$\n",
    "\n",
    "- Supervised Contrastive learning loss\n",
    "$$\n",
    "\\mathcal{L}_\\text{S_cl} = -\\frac{1}{T}\\sum_{i=1}^{N}\\sum_{j=1}^{N} \\boldsymbol{1_{yi=yj}}\\enspace log \\frac{e^{sim(hi,hj) / \\tau}}{\\sum_{n=1}^{N} e^{sim(hi,hn) / \\tau}}\n",
    "$$\n",
    "     * detail \n",
    "       * ui ~ sentence i \n",
    "       * hi ~ BERT(ui) in our case using Roberta as a encoder\n",
    "       * hi : (batch_size,sequence_len,embed_size)\n",
    "       * hi is the output of model which is last hidden layers before classifier head in the model architecture\n",
    "       * 1yi=yj ~ we select only the sample that come from the same class to compute in each i and j\n",
    "       * T ~ the number of pairs that come from the same classes\n",
    "       * $\\tau$ ~ temperature parameter\n",
    "       * Sim(x1,x2) : cosine similarity [-1, 1]\n",
    "       - $\\lambda'$ is just weighted of cross entropy loss \n",
    "       * Sim function is the cosine similarity \n",
    "       * N ~ the number of samples in a batch\n",
    "$$\n",
    "sim(A,B) = \\cos{(\\theta)} = \\frac{A\\cdot B}{|\\!|A|\\!||\\!|B|\\!|}\n",
    "$$\n",
    "\n",
    "\n",
    "- Loss total\n",
    "$$\n",
    "  \\mathcal{L}_\\text{total} = \\mathcal{L}_\\text{s_cl} + \\lambda ' \\mathcal{L}_{CE}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31687e10-7a63-4592-94a3-dae140324863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AdamW\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from scipy.spatial.distance import cosine\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'simcse')))\n",
    "from sim_utils import load_examples, Inputexample, CustomTextDataset, freeze_layers, test\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, AutoModel, AutoTokenizer\n",
    "from loss import Similarity, create_supervised_pair, supervised_contrasive_loss\n",
    " \n",
    "#comment this if you are not using puffer\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd826aab-d360-45d5-91f0-cffa0333ceb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec2f966-fc6a-479f-bf71-e6bfe8b37743",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train With Combine loss between Cross Entropy and SuperVised Contrastive loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7446a5-a283-4a58-a327-daa46974d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive_learnig(model,optimizer,train_loader,tokenizer,valid_loader,device,epochs:int=30):\n",
    "    \n",
    "    \n",
    "    train_loss_hist = [] \n",
    "    valid_loss_hist = []\n",
    "    \n",
    "    train_acc_hist = []\n",
    "    valid_acc_hist = []\n",
    "    \n",
    "\n",
    "    test_acc = []\n",
    "\n",
    "    min_valid_loss = np.inf\n",
    "    skip_time = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation\n",
    "    for e in range(epochs):  # loop over the dataset multiple times\n",
    " \n",
    "        model.train()\n",
    "        correct = 0\n",
    "        running_loss = 0.0\n",
    "       \n",
    "\n",
    "        for (idx, batch) in enumerate(train_loader):\n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "            #assert len(np.unique(batch[\"Class\"])) == len(batch[\"Class\"])  \n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "             # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = model(**inputs,labels=labels,output_hidden_states=True)     \n",
    "        \n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            last_hidden_states = hidden_states[12]\n",
    "\n",
    "            # https://stackoverflow.com/questions/63040954/how-to-extract-and-use-bert-encodings-of-sentences-for-text-similarity-among-sen \n",
    "            # (batch_size,seq_len,embed_dim)\n",
    "            h = last_hidden_states[:,0,:]\n",
    "\n",
    "            # create pair samples\n",
    "            T, h_i, h_j, idx_yij = create_supervised_pair(h,batch['Class'],debug=False)\n",
    "\n",
    "            if h_i is None:\n",
    "                print(\"skip this batch\")\n",
    "                skip_time +=1 \n",
    "                continue\n",
    "\n",
    "            # supervised contrastive loss \n",
    "            \n",
    "            loss_s_cl = supervised_contrasive_loss(device,h_i, h_j, h, T,temp=temp,idx_yij=idx_yij,debug=False)\n",
    "\n",
    "            # cross entropy loss\n",
    "            loss_classify, logits = outputs[:2]\n",
    "\n",
    "            # loss total\n",
    "            loss = loss_s_cl + (lamda * loss_classify )\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate nums of correction \n",
    "            correct += (torch.max(logits,dim=-1)[1] == labels).sum()\n",
    "\n",
    "        \n",
    "        train_loss_hist.append(running_loss/len(train_data))\n",
    "        train_acc_hist.append(correct/len(train_data))\n",
    "        \n",
    "        \n",
    "        print(f'======  Epoch {e+1} ====== ')\n",
    "        print(f' Training Loss: {running_loss/len(train_data)}, \\t\\t Training acc: {correct/len(train_data)}')\n",
    "        \n",
    "        print(\"train correct : \",correct)\n",
    "        print(\"train total :\",len(train_data))\n",
    "        \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        model.eval()     # Optional when not using Model Specific layer\n",
    "        log_correct = []\n",
    "        \n",
    "        \n",
    "        for (idx, batch) in enumerate(valid_loader):\n",
    "            \n",
    "            sentence = batch[\"Text\"]\n",
    "            inputs = tokenizer(sentence,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "            #assert len(np.unique(batch[\"Class\"])) < len(batch[\"Class\"])  \n",
    "            # move parameter to device\n",
    "            inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "\n",
    "            # map string labels to class idex\n",
    "            labels = [label_maps[stringtoId] for stringtoId in (batch['Class'])]\n",
    "\n",
    "            # convert list to tensor\n",
    "            labels = torch.tensor(labels).unsqueeze(0)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "             # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            outputs = model(**inputs,labels=labels,output_hidden_states=True)     \n",
    "        \n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "            last_hidden_states = hidden_states[12]\n",
    "\n",
    "            # https://stackoverflow.com/questions/63040954/how-to-extract-and-use-bert-encodings-of-sentences-for-text-similarity-among-sen \n",
    "            # (batch_size,seq_len,embed_dim)\n",
    "            h = last_hidden_states[:,0,:]\n",
    "\n",
    "            # create pair samples\n",
    "            T, h_i, h_j, idx_yij = create_supervised_pair(h,batch['Class'],debug=False)\n",
    "\n",
    "            # if h_i is None:\n",
    "            #     print(\"skip this batch\")\n",
    "            #     skip_time +=1\n",
    "            #     continue\n",
    "\n",
    "            # supervised contrastive loss \n",
    "            loss_s_cl = supervised_contrasive_loss(device,h_i, h_j, h, T,temp=temp,idx_yij=idx_yij,debug=False)\n",
    "\n",
    "            # cross entropy loss\n",
    "            loss_classify, logits = outputs[:2]\n",
    "\n",
    "            # loss total\n",
    "            loss = loss_s_cl + (lamda * loss_classify )\n",
    "            \n",
    "            # Calculate Loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            #calculate nums of correction \n",
    "            correct += (torch.max(logits,dim=-1)[1] == labels).sum()\n",
    "            \n",
    "        # add code to logging\n",
    "        valid_loss_hist.append(running_loss/len(valid_data))\n",
    "        valid_acc_hist.append(correct/len(valid_data))\n",
    "        \n",
    "        print(f' Validation Loss: {running_loss/len(valid_data)}, \\t\\t Validation acc: {correct/len(valid_data)}')\n",
    "        \n",
    "        print(\"valid correct : \",correct)\n",
    "        print(\"valid total :\",len(valid_data))\n",
    "       \n",
    "        \n",
    "        \n",
    "        # save best current model \n",
    "        if min_valid_loss > (running_loss/len(valid_data)):\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{running_loss/len(valid_data):.6f}) \\t Saving The Model')\n",
    "            min_valid_loss = running_loss/len(valid_data) \n",
    "            torch.save(model.state_dict(), 'saved_model.pth')\n",
    "            \n",
    "       \n",
    "            \n",
    "    return (train_acc_hist, train_loss_hist), (valid_acc_hist, valid_loss_hist)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96023b6-4a7f-4f25-b175-bf5cdb1136a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f656717-e0e7-4e51-8541-6ab4c2b25c59",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bd1ea-56ff-4e4c-a2ef-36787f04c2d1",
   "metadata": {},
   "source": [
    "### The Aim of these training is to fine tuning on few shot setting on text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9361f0ce-c287-4cb2-9ebd-d8487c3f35bc",
   "metadata": {},
   "source": [
    "Path example of train, validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e365ee63-74ce-421f-964b-b644b9fdc131",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "\n",
    "valid_samples = []\n",
    "valid_labels = []\n",
    "\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "\n",
    "embed_dim = 768\n",
    "batch_size = 16 \n",
    "lr=2e-3  # you can adjust \n",
    "temp = 0.3  # you can adjust \n",
    "lamda = 0.01  # you can adjust  \n",
    "skip_time = 0 # the number of time that yi not equal to yj in supervised contrastive loss equation \n",
    "data_name = 'BANKING77'\n",
    "model_name = 'simcse_sup'\n",
    "shot_name = 'train_5'\n",
    "exp_name = f'{model_name}_lr={lr}_t={temp}_{data_name}_{shot_name}'\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58a34c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train path :  ../../../../Thesis/BANKING77/train_5/\n",
      "valid path :  ../../../../Thesis/BANKING77/valid/\n",
      "test path  :  ../../../../Thesis/BANKING77/test/\n",
      "experiment code name : simcse_sup_lr=0.002_t=0.3_BANKING77_train_5\n"
     ]
    }
   ],
   "source": [
    "path_shot = f'../../../../Thesis/{data_name}/{shot_name}/'\n",
    "valid_path = f'../../../../Thesis/{data_name}/valid/'\n",
    "test_path = f'../../../../Thesis/{data_name}/test/'\n",
    "print(\"train path : \",path_shot)\n",
    "print(\"valid path : \",valid_path)\n",
    "print(\"test path  : \",test_path)\n",
    "print(\"experiment code name :\",exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24b0c99-e4fe-4be3-a9dc-15d707ee4be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== small train set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 385\n",
      "===== validation set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 1540\n",
      "===== test set ====\n",
      "Train on Combine between Supervised Contrastive and Cross Entropy loss\n",
      "len of dataset : 3080\n"
     ]
    }
   ],
   "source": [
    "# Download data fewshot \n",
    "# https://downgit.github.io/#/home?url=https:%2F%2Fgithub.com%2Fjianguoz%2FFew-Shot-Intent-Detection%2Ftree%2Fmain%2FDatasets%2FHWU64\n",
    "\n",
    "# load data\n",
    "train_samples = load_examples(path_shot)\n",
    "valid_samples = load_examples(valid_path)\n",
    "test_samples = load_examples(test_path)\n",
    "\n",
    "\n",
    "print(\"===== small train set ====\")\n",
    "\n",
    "for i in range(len(train_samples)):\n",
    "    data.append(train_samples[i].text)\n",
    "    labels.append(train_samples[i].label)\n",
    "\n",
    "\n",
    "train_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "train_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"===== validation set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(valid_samples)):\n",
    "    data.append(valid_samples[i].text)\n",
    "    labels.append(valid_samples[i].label)\n",
    "\n",
    "valid_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "valid_loader = DataLoader(valid_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "print(\"===== test set ====\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "    \n",
    "for i in range(len(test_samples)):\n",
    "    data.append(test_samples[i].text)\n",
    "    labels.append(test_samples[i].label)\n",
    "\n",
    "test_data = CustomTextDataset(labels,data,batch_size=batch_size,repeated_label=True)\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# got the number of unique classes from dataset\n",
    "num_class = len(np.unique(np.array(labels)))\n",
    "\n",
    "# get text label of uniqure classes\n",
    "unique_label = np.unique(np.array(labels))\n",
    "\n",
    "# map text label to index classes\n",
    "label_maps = {unique_label[i]: i for i in range(len(unique_label))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc0bf94-7934-428a-affc-37b93078424d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c9979-022c-45a4-acc5-2a4c7637b4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f36ddd412784f18b7b873b905da4430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9461f57530034865a58475a7c31cad30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d467cf40db4ef2adcc27d3edb6569c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4028c90e4f6243768a59935fa123f1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "config = AutoConfig.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n",
    "config.num_labels = num_class\n",
    "simcse = AutoModelForSequenceClassification.from_pretrained(\"princeton-nlp/sup-simcse-bert-base-uncased\",config=config)\n",
    "simcse = freeze_layers(simcse,freeze_layers_count=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694d0244-0833-45a8-8c47-c8fd5c3ee860",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= AdamW(simcse.parameters(), lr=lr)\n",
    "simcse = simcse.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee41e2-8569-4210-aaad-491c3b0dd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log, valid_log = train_contrastive_learnig(simcse,optimizer,train_loader,tokenizer,valid_loader,device,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig.suptitle('Show t 4.1 by freezing 9 layers')\n",
    "ax1.plot(torch.tensor(train_log[0] , device = 'cpu').tolist())\n",
    "ax1.plot(torch.tensor(valid_log[0] , device = 'cpu').tolist())\n",
    "ax1.legend(['train_acc','validation_acc'])\n",
    "ax2.plot(torch.tensor(train_log[1] , device = 'cpu').tolist())\n",
    "ax2.plot(torch.tensor(valid_log[1] , device = 'cpu').tolist())\n",
    "ax2.legend(['train_loss','validation_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02deb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(model,test_loader=test_loader,data_size=len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2771a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy : {100 * test_acc} %') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5264623-e494-4b7a-9c3d-ca1d58d9b797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
